{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd933247-e58c-4bf1-b4d4-1d84d26b8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All fuction preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf4ca6-2fd6-4370-a45d-87eafb286fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_crop_positions(total, crop_size=1024, overlap=0.25, min_overlap=256):\n",
    "    \"\"\"\n",
    "    Generate a list of crop positions.\n",
    "\n",
    "    :param total: The total width or height of the image.\n",
    "    :param crop_size: The size of the crop (default 1024).\n",
    "    :param overlap: The overlap ratio (default 0.25).\n",
    "    :param min_overlap: The minimum overlap in pixels (default 512).\n",
    "    :return: A list of crop positions, e.g., [[start1, end1], [start2, end2], ...].\n",
    "    \"\"\"\n",
    "    if total <= crop_size:\n",
    "        # If total dimension is smaller than crop size, return the full range\n",
    "        return [[0, total]]\n",
    "\n",
    "    # Calculate step size based on overlap ratio\n",
    "    step_size = int(crop_size * (1 - overlap))\n",
    "    positions = []\n",
    "    pos = 0\n",
    "\n",
    "    # Generate positions with calculated step size\n",
    "    while pos + crop_size <= total:\n",
    "        positions.append([pos, pos + crop_size])\n",
    "        pos += step_size\n",
    "\n",
    "    # Handle remaining portion of the image\n",
    "    if positions:\n",
    "        remaining = total - positions[-1][1]\n",
    "    else:\n",
    "        remaining = total\n",
    "\n",
    "    if remaining > 0:\n",
    "        if remaining < min_overlap:\n",
    "            # Adjust last position if remaining is smaller than min_overlap\n",
    "            positions[-1][1] = total\n",
    "        else:\n",
    "            # Add new position for remaining portion if it's significant\n",
    "            new_start = positions[-1][1] - (crop_size - step_size)\n",
    "            if new_start < 0:\n",
    "                new_start = 0\n",
    "            positions.append([new_start, total])\n",
    "\n",
    "    return positions\n",
    "\n",
    "def crop_image_with_overlap(image, output_dir, crop_size=1024, overlap=0.25, min_overlap=256):\n",
    "    \"\"\"\n",
    "    Crop the image based on the overlap ratio.\n",
    "\n",
    "    :param image_path: Path to the input image.\n",
    "    :param output_dir: Output directory.\n",
    "    :param crop_size: Crop size (default 1024).\n",
    "    :param overlap: Overlap ratio (default 0.25).\n",
    "    :param min_overlap: Minimum overlap in pixels (default 512).\n",
    "    :return: Returns the number of rows and columns of crops.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "\n",
    "    # Generate crop positions for both dimensions\n",
    "    crop_positions_width = generate_crop_positions(width, crop_size, overlap, min_overlap)\n",
    "    crop_positions_height = generate_crop_positions(height, crop_size, overlap, min_overlap)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Base name for output files\n",
    "    image_name = 'temp'\n",
    "    count = 1\n",
    "\n",
    "    # Process each crop position\n",
    "    for row, (top, bottom) in enumerate(crop_positions_height):\n",
    "        for col, (left, right) in enumerate(crop_positions_width):\n",
    "            # Perform the actual cropping\n",
    "            cropped_image = image[top:bottom, left:right]\n",
    "\n",
    "            # Convert numpy array to PIL Image\n",
    "            cropped_image_pil = Image.fromarray(cropped_image)\n",
    "\n",
    "            # Generate output path\n",
    "            output_path = os.path.join(output_dir, f\"{image_name}_{row + 1}_{col + 1}.png\")\n",
    "\n",
    "            # Save the cropped image\n",
    "            cropped_image_pil.save(output_path)\n",
    "            count += 1\n",
    "\n",
    "    return len(crop_positions_height), len(crop_positions_width)\n",
    "\n",
    "def mask_nms_with_scores(cls_logits, mask_logits, scores, iou_thr=0.5):\n",
    "    \"\"\"\n",
    "    Use NMS to filter overlapping masks, returning the indices of the retained queries.\n",
    "\n",
    "    Args:\n",
    "        cls_logits (Tensor): Classification logits, shape (num_queries, num_classes).\n",
    "        mask_logits (Tensor): Mask logits, shape (num_queries, h, w).\n",
    "        scores (Tensor): Scores for each query, shape (num_queries,).\n",
    "        iou_thr (float): IoU threshold.\n",
    "\n",
    "    Returns:\n",
    "        keep_indices (List[int]): Indices of the retained queries (sorted in ascending order).\n",
    "    \"\"\"\n",
    "    # Convert mask logits to probabilities\n",
    "    mask_pred = mask_logits.sigmoid()\n",
    "\n",
    "    # Early return if no masks\n",
    "    num_masks = mask_pred.shape[0]\n",
    "    if num_masks == 0:\n",
    "        return []\n",
    "\n",
    "    # Flatten masks for IoU calculation\n",
    "    mask_bool = (mask_pred >= 0.5).view(num_masks, -1)\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = torch.matmul(mask_bool.float(), mask_bool.float().transpose(0, 1))\n",
    "    union = mask_bool.sum(dim=1).view(-1, 1) + mask_bool.sum(dim=1).view(1, -1) - intersection\n",
    "    iou_matrix = intersection / union\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    sorted_indices = torch.argsort(scores, descending=True)\n",
    "    keep_indices = []\n",
    "\n",
    "    # NMS filtering\n",
    "    for idx in sorted_indices:\n",
    "        # Check overlap with kept indices\n",
    "        if all(iou_matrix[idx, kept_idx] <= iou_thr for kept_idx in keep_indices):\n",
    "            keep_indices.append(idx.item())\n",
    "\n",
    "    # Return sorted indices\n",
    "    keep_indices.sort()\n",
    "    return keep_indices\n",
    "\n",
    "def aggregate_mask_logits(labels_final, mask_logits_final, num_classes=5):\n",
    "    \"\"\"\n",
    "    Aggregate mask logits of the same class to generate masks for each class.\n",
    "\n",
    "    Args:\n",
    "        labels_final (Tensor): Filtered labels, shape (num_queries,).\n",
    "        mask_logits_final (Tensor): Filtered mask logits, shape (num_queries, H, W).\n",
    "        num_classes (int): Number of classes (default 5).\n",
    "\n",
    "    Returns:\n",
    "        class_masks (Tensor): Aggregated masks for each class, shape (num_classes, H, W).\n",
    "    \"\"\"\n",
    "    # Initialize output tensor with negative infinity\n",
    "    h, w = mask_logits_final.shape[-2:]\n",
    "    class_masks = torch.full((num_classes, h, w), -float('inf'), device=mask_logits_final.device)\n",
    "\n",
    "    # Process each class separately\n",
    "    for class_id in range(num_classes):\n",
    "        # Find masks belonging to current class\n",
    "        class_mask_indices = (labels_final == class_id)\n",
    "        \n",
    "        if class_mask_indices.any():\n",
    "            # Get all masks for this class\n",
    "            class_mask_logits = mask_logits_final[class_mask_indices]\n",
    "            \n",
    "            # Take element-wise maximum\n",
    "            class_mask_max = torch.max(class_mask_logits, dim=0).values\n",
    "            \n",
    "            # Store result\n",
    "            class_masks[class_id] = class_mask_max\n",
    "\n",
    "    return class_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac365e-3dbb-453a-b726-9b3cafcf9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight matrix generation with cosine transition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create a base weight matrix for corners (top-left corner)\n",
    "def create_corner_base_weight_matrix(height, width, overlap):\n",
    "    \"\"\"\n",
    "    Create a base weight matrix for the top-left corner mask.\n",
    "    \"\"\"\n",
    "    weight = np.ones((height, width))\n",
    "    x = np.linspace(0, np.pi / 2, overlap)  # For cosine transition\n",
    "    cos_weights = np.cos(x) ** 2  # Cosine weights\n",
    "\n",
    "    # Right overlap\n",
    "    weight[:, -overlap:] *= cos_weights\n",
    "    # Bottom overlap\n",
    "    weight[-overlap:, :] *= cos_weights[:, None]\n",
    "    # Bottom-right corner overlap\n",
    "    weight[-overlap:, -overlap:] *= np.outer(cos_weights, cos_weights)\n",
    "\n",
    "    return weight\n",
    "\n",
    "# Function to create a base weight matrix for edges (top edge)\n",
    "def create_edge_base_weight_matrix(height, width, overlap, edge_type):\n",
    "    \"\"\"\n",
    "    Create a base weight matrix for the edge mask.\n",
    "    edge_type: 'top', 'bottom', 'left', 'right'\n",
    "    \"\"\"\n",
    "    weight = np.ones((height, width))\n",
    "    x = np.linspace(0, np.pi / 2, overlap)  # For cosine transition\n",
    "    cos_weights = np.cos(x) ** 2  # Cosine weights\n",
    "\n",
    "    if edge_type == 'top':\n",
    "        # Left overlap\n",
    "        weight[:, :overlap] *= cos_weights[::-1]\n",
    "        # Right overlap\n",
    "        weight[:, -overlap:] *= cos_weights\n",
    "        # Bottom overlap\n",
    "        weight[-overlap:, :] *= cos_weights[:, None]\n",
    "    elif edge_type == 'bottom':\n",
    "        # Left overlap\n",
    "        weight[:, :overlap] *= cos_weights[::-1]\n",
    "        # Right overlap\n",
    "        weight[:, -overlap:] *= cos_weights\n",
    "        # Top overlap\n",
    "        weight[:overlap, :] *= cos_weights[::-1][:, None]\n",
    "    elif edge_type == 'left':\n",
    "        # Top overlap\n",
    "        weight[:overlap, :] *= cos_weights[::-1][:, None]\n",
    "        # Bottom overlap\n",
    "        weight[-overlap:, :] *= cos_weights[:, None]\n",
    "        # Right overlap\n",
    "        weight[:, -overlap:] *= cos_weights\n",
    "    elif edge_type == 'right':\n",
    "        # Top overlap\n",
    "        weight[:overlap, :] *= cos_weights[::-1][:, None]\n",
    "        # Bottom overlap\n",
    "        weight[-overlap:, :] *= cos_weights[:, None]\n",
    "        # Left overlap\n",
    "        weight[:, :overlap] *= cos_weights[::-1]\n",
    "\n",
    "    return weight\n",
    "\n",
    "# Function to create a base weight matrix for the center\n",
    "def create_center_weight_matrix(height, width, overlap):\n",
    "    \"\"\"\n",
    "    Create a weight matrix for the center mask.\n",
    "    \"\"\"\n",
    "    weight = np.ones((height, width))\n",
    "    x = np.linspace(0, np.pi / 2, overlap)  # For cosine transition\n",
    "    cos_weights = np.cos(x) ** 2  # Cosine weights\n",
    "\n",
    "    # Apply weights for all four sides and corners\n",
    "    weight[:overlap, :] *= cos_weights[::-1][:, None]  # Top overlap\n",
    "    weight[-overlap:, :] *= cos_weights[:, None]  # Bottom overlap\n",
    "    weight[:, :overlap] *= cos_weights[::-1]  # Left overlap\n",
    "    weight[:, -overlap:] *= cos_weights  # Right overlap\n",
    "\n",
    "    return weight\n",
    "\n",
    "# Function to create weight matrix based on position (x, y) and grid size (x_max, y_max)\n",
    "def create_weight_matrix(x, y, x_max, y_max, height, width, overlap):\n",
    "    \"\"\"\n",
    "    Create a weight matrix for a given mask position (x, y) in a grid of size (x_max, y_max).\n",
    "    The weight matrix size is determined by the input image size (height, width).\n",
    "    \"\"\"\n",
    "    # Determine the position type: corner, edge, or center\n",
    "    is_corner = (x == 1 and y == 1) or (x == 1 and y == y_max) or (x == x_max and y == 1) or (x == x_max and y == y_max)\n",
    "    is_edge = not is_corner and (x == 1 or x == x_max or y == 1 or y == y_max)\n",
    "    is_center = not is_corner and not is_edge\n",
    "\n",
    "    if is_corner:\n",
    "        # Use the corner base weight matrix\n",
    "        if x == 1 and y == 1:  # Top-left corner\n",
    "            return create_corner_base_weight_matrix(height, width, overlap)\n",
    "        elif x == 1 and y == y_max:  # Top-right corner\n",
    "            weight = create_corner_base_weight_matrix(height, width, overlap)\n",
    "            return np.fliplr(weight)\n",
    "        elif x == x_max and y == 1:  # Bottom-left corner\n",
    "            weight = create_corner_base_weight_matrix(height, width, overlap)\n",
    "            return np.flipud(weight)\n",
    "        elif x == x_max and y == y_max:  # Bottom-right corner\n",
    "            weight = create_corner_base_weight_matrix(height, width, overlap)\n",
    "            return np.flipud(np.fliplr(weight))\n",
    "    elif is_edge:\n",
    "        # Use the edge base weight matrix\n",
    "        if x == 1:  # Top edge\n",
    "            return create_edge_base_weight_matrix(height, width, overlap, 'top')\n",
    "        elif x == x_max:  # Bottom edge\n",
    "            return create_edge_base_weight_matrix(height, width, overlap, 'bottom')\n",
    "        elif y == 1:  # Left edge\n",
    "            return create_edge_base_weight_matrix(height, width, overlap, 'left')\n",
    "        elif y == y_max:  # Right edge\n",
    "            return create_edge_base_weight_matrix(height, width, overlap, 'right')\n",
    "    else:\n",
    "        # Use the center weight matrix\n",
    "        return create_center_weight_matrix(height, width, overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a241d665-ffb7-4540-9717-7b6f7af95908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c814645-ece2-408f-a4c1-85b65a4de91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Web\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "D:\\Anaconda\\envs\\Web\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: epoch_100.pth\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Initialize model\n",
    "config_file = 'mask2former_swin-l-p4-w12-384_8xb2-lsj-100e_coco-1227.py'\n",
    "checkpoint_file = 'epoch_100.pth'\n",
    "device = 'cuda:0'\n",
    "model = init_detector(config_file, checkpoint_file, device=device)\n",
    "model.eval()\n",
    "class_names = model.dataset_meta.get('classes', [])\n",
    "num_classes = len(class_names)\n",
    "risk_thresholds = [(3825, 43333), (3610, 70096), (4415, 15355), (1552, 13158), (714, 4823)]\n",
    "\n",
    "def visualize_results(big_mask_logits, image_large, class_names, risk_thresholds):\n",
    "    \"\"\"\n",
    "    Visualize results: Generate probability map, binary map, boundary map and segmentation map.\n",
    "    \n",
    "    Args:\n",
    "        big_mask_logits (Tensor): Mask logits for the large image, shape (num_classes, H, W).\n",
    "        image_large (numpy.ndarray): Original large image, shape (H, W, 3).\n",
    "        class_names (list): List of class names.\n",
    "        risk_thresholds (list): Area thresholds for each class in format [(low1, high1), (low2, high2), ...].\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing the following key-value pairs:\n",
    "            - \"probability\": Probability map (OpenCV format).\n",
    "            - \"binary\": Binary map (PIL format).\n",
    "            - \"boundary\": Boundary map (PIL format).\n",
    "            - \"segmentation\": Segmentation map (PIL format).\n",
    "            - \"original\": Original large image (numpy.ndarray format).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate probability map and binary map\n",
    "    max_probs, _ = torch.max(big_mask_logits, dim=0)\n",
    "\n",
    "    # Handle NaN and Inf values\n",
    "    if torch.isnan(max_probs).any() or torch.isinf(max_probs).any():\n",
    "        max_probs = torch.nan_to_num(max_probs, nan=np.nan, posinf=np.nan, neginf=np.nan)  # Replace Inf with NaN\n",
    "    \n",
    "    probability_map = torch.sigmoid(max_probs).cpu().numpy()  # For binary classification\n",
    "    # probability_map = torch.softmax(max_probs, dim=0).cpu().numpy()  # For multi-class classification\n",
    "    \n",
    "    # Interpolate to fill NaN values\n",
    "    if np.isnan(probability_map).any():\n",
    "        valid_mask = ~np.isnan(probability_map)  # Mask of valid values\n",
    "        points = np.argwhere(valid_mask)  # Coordinates of valid values\n",
    "        values = probability_map[valid_mask]  # Valid values\n",
    "        grid_x, grid_y = np.mgrid[0:probability_map.shape[0], 0:probability_map.shape[1]]\n",
    "        probability_map = griddata(points, values, (grid_x, grid_y), method='nearest')\n",
    "    \n",
    "    # Scale probability map to [0, 255] and convert to uint8\n",
    "    probability_map = (probability_map * 255).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Apply color map\n",
    "    probability_image = cv2.applyColorMap(probability_map, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Generate binary map\n",
    "    binary_map = (probability_map > 0).astype(np.uint8) * 255\n",
    "    binary_image = Image.fromarray(binary_map)\n",
    "\n",
    "    # Generate boundary map and segmentation map\n",
    "    logits = big_mask_logits.cpu().numpy()\n",
    "    colors = [\n",
    "        (255, 222, 7),    # #07deff\n",
    "        (61, 207, 255),   # #ffcf3d\n",
    "        (255, 74, 74),    # #4a4aff\n",
    "        (127, 85, 0),     # #00557f\n",
    "        (87, 15, 255)     # #ff0f57\n",
    "    ]\n",
    "    bbox_colors = [(0, 255, 0), (0, 165, 255), (0, 0, 255)]  # Green, orange, red\n",
    "\n",
    "    output_image = image_large.copy()\n",
    "    segmentation_image = image_large.copy()\n",
    "\n",
    "    # Store annotation data for each valid contour\n",
    "    annotations = []  \n",
    "\n",
    "    # Process each class\n",
    "    for class_idx in range(len(class_names)):\n",
    "        mask = logits[class_idx] > 0\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area < 50:  # Skip small contours\n",
    "                continue\n",
    "                \n",
    "            # Get segmentation coordinates (flattened contour points)\n",
    "            segmentation = contour.flatten().tolist()\n",
    "            if len(segmentation) < 6 or len(segmentation) % 2 != 0:\n",
    "                continue  # Skip if insufficient points or not in (x,y) pairs\n",
    "    \n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Store annotation data\n",
    "            annotations.append({\n",
    "                \"class_id\": class_idx + 1,  # COCO format starts class IDs from 1\n",
    "                \"segmentation\": segmentation,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"area\": float(area)\n",
    "            })\n",
    "    \n",
    "            # Draw boundary\n",
    "            cv2.drawContours(output_image, [contour], -1, colors[class_idx], 2)\n",
    "    \n",
    "            # Generate bounding box\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "            # Set bbox color based on area thresholds\n",
    "            if area < risk_thresholds[class_idx][0]:\n",
    "                bbox_color = bbox_colors[0]  # Green\n",
    "            elif area < risk_thresholds[class_idx][1]:\n",
    "                bbox_color = bbox_colors[1]  # Orange\n",
    "            else:\n",
    "                bbox_color = bbox_colors[2]  # Red\n",
    "    \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(output_image, (x, y), (x + w, y + h), bbox_color, 2)\n",
    "    \n",
    "            # Add class label in top-left corner of bbox\n",
    "            text = class_names[class_idx]\n",
    "            font_scale = 0.8  # Font size\n",
    "            thickness = 2  # Font thickness\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "    \n",
    "            # Calculate text position (inside bbox top-left)\n",
    "            text_x = x + 5  # 5px from left edge\n",
    "            text_y = y + 20  # 20px from top edge\n",
    "    \n",
    "            # Ensure text stays within bbox\n",
    "            if text_y - text_height < y:  # If text goes above bbox\n",
    "                text_y = y + text_height + 5  # Move text down\n",
    "            if text_x + text_width > x + w:  # If text goes beyond right edge\n",
    "                text_x = x + w - text_width - 5  # Move text left\n",
    "    \n",
    "            # Draw text background\n",
    "            cv2.rectangle(output_image, (text_x, text_y - text_height), (text_x + text_width, text_y), bbox_color, -1)  # -1 for filled\n",
    "    \n",
    "            # Draw text\n",
    "            cv2.putText(output_image, text, (text_x, text_y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "            # Draw segmentation (filled contour)\n",
    "            overlay = segmentation_image.copy()\n",
    "            cv2.drawContours(overlay, [contour], -1, colors[class_idx], -1)  # Fill contour\n",
    "            alpha = 0.5  # 50% transparency\n",
    "            cv2.addWeighted(overlay, alpha, segmentation_image, 1 - alpha, 0, segmentation_image)\n",
    "\n",
    "    # Convert OpenCV images to PIL format\n",
    "    boundary_map = Image.fromarray(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "    segmentation_map = Image.fromarray(cv2.cvtColor(segmentation_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return {\n",
    "        \"probability\": probability_image,\n",
    "        \"binary\": binary_image,\n",
    "        \"boundary\": boundary_map,\n",
    "        \"segmentation\": segmentation_map,\n",
    "        \"original\": image_large,\n",
    "        \"json_coords\": annotations,  # Contour data for annotations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9454eb-aeb9-4318-a5ca-a1a1300be774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "m = None  # Global variable to store mask logits\n",
    "\n",
    "def process_image(image, confidence_threshold=0.5, crack_threshold=0.01, iou_thr_nms=0.2, \n",
    "                 crop_sizes=[1000, 1400, 1800], overlap=0.5, min_overlap=256):\n",
    "    \"\"\"\n",
    "    Main function: Process large image, generate mask logits and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input large image with shape (H, W, 3).\n",
    "        confidence_threshold (float): Confidence threshold (default 0.5).\n",
    "        crack_threshold (float): Special threshold for crack detection (default 0.01).\n",
    "        iou_thr_nms (float): IoU threshold for NMS (default 0.2).\n",
    "        crop_sizes (list): List of crop sizes to process (default [1000, 1400, 1800]).\n",
    "        overlap (float): Overlap ratio between crops (default 0.5).\n",
    "        min_overlap (int): Minimum overlap in pixels (default 256).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing visualization results with keys:\n",
    "            - \"probability\": Probability map (OpenCV format).\n",
    "            - \"binary\": Binary map (PIL format).\n",
    "            - \"boundary\": Boundary map (PIL format).\n",
    "            - \"segmentation\": Segmentation map (PIL format).\n",
    "            - \"original\": Original image (numpy.ndarray format).\n",
    "    \"\"\"\n",
    "    global m  # Access global variable\n",
    "    temp_dir = 'temp_test2'  # Temporary directory for cropped images\n",
    "    image_large = image  # Store original image\n",
    "    big_mask_logits_list = []  # List to store mask logits from different crop sizes\n",
    "\n",
    "    # Process image with each crop size\n",
    "    for crop_size in crop_sizes:\n",
    "        # Initialize tensor for aggregated mask logits\n",
    "        big_mask_logits = torch.zeros((num_classes, image_large.shape[0], image_large.shape[1]), \n",
    "                         device=device)\n",
    "\n",
    "        # Generate crop positions for current size\n",
    "        crop_positions_height = generate_crop_positions(image_large.shape[0], crop_size, \n",
    "                                                      overlap, min_overlap)\n",
    "        crop_positions_width = generate_crop_positions(image_large.shape[1], crop_size, \n",
    "                                                     overlap, min_overlap)\n",
    "        \n",
    "        # Crop image and get crop positions\n",
    "        rows, cols = crop_image_with_overlap(image_large, temp_dir, crop_size, \n",
    "                                           overlap, min_overlap)\n",
    "\n",
    "        try:\n",
    "            # Process each cropped image\n",
    "            for filename in tqdm(os.listdir(temp_dir)):\n",
    "                if filename.endswith('.png'):\n",
    "                    try:\n",
    "                        # Parse coordinates from filename (format: prefix_x_y.png)\n",
    "                        parts = filename.split('_')\n",
    "                        if len(parts) >= 3:\n",
    "                            x = int(parts[-2])  # Row index\n",
    "                            y = int(parts[-1].split('.')[0])  # Column index\n",
    "                        else:\n",
    "                            # Fallback: extract digits from filename\n",
    "                            x, y = map(int, ''.join(filter(str.isdigit, filename)).split())\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Cannot parse coordinates from filename {filename}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Validate coordinates\n",
    "                    if x - 1 >= len(crop_positions_height) or y - 1 >= len(crop_positions_width):\n",
    "                        print(f\"Coordinates ({x}, {y}) out of range, skipping image\")\n",
    "                        continue\n",
    "\n",
    "                    # Perform inference on cropped image\n",
    "                    img_path = os.path.join(temp_dir, filename)\n",
    "                    image = cv2.imread(img_path)\n",
    "                    result = inference_detector(model, image)\n",
    "\n",
    "                    # Get detection results\n",
    "                    labels = result.pred_instances['labels']\n",
    "                    mask_logits = result.pred_instances['mask_logits']\n",
    "                    scores = result.pred_instances['scores']\n",
    "\n",
    "                    # Apply confidence thresholds (special lower threshold for cracks)\n",
    "                    valid_indices = torch.where(\n",
    "                        (scores > confidence_threshold) | \n",
    "                        ((labels == 4) & (scores > crack_threshold))\n",
    "                    )[0]\n",
    "                    \n",
    "                    # Apply NMS to filter overlapping masks\n",
    "                    keep_indices = mask_nms_with_scores(\n",
    "                        labels[valid_indices], \n",
    "                        mask_logits[valid_indices], \n",
    "                        scores[valid_indices], \n",
    "                        iou_thr=iou_thr_nms\n",
    "                    )\n",
    "                    labels_final = labels[valid_indices][keep_indices]\n",
    "                    mask_logits_final = mask_logits[valid_indices][keep_indices]\n",
    "                    \n",
    "                    # Aggregate masks by class\n",
    "                    class_masks = aggregate_mask_logits(labels_final, mask_logits_final, num_classes)\n",
    "\n",
    "                    # Create weight matrix for blending\n",
    "                    weight_matrix = create_weight_matrix(\n",
    "                        x, y, rows, cols, \n",
    "                        image.shape[0], image.shape[1], \n",
    "                        overlap=256\n",
    "                    )\n",
    "                    weight_matrix = torch.from_numpy(weight_matrix.copy()).float().to(device)\n",
    "        \n",
    "                    # Blend masks into the large image\n",
    "                    for class_id in range(num_classes):\n",
    "                        class_mask = class_masks[class_id] * weight_matrix\n",
    "                        start_x, end_x = crop_positions_height[x - 1]\n",
    "                        start_y, end_y = crop_positions_width[y - 1]\n",
    "                        big_mask_logits[class_id, start_x:end_x, start_y:end_y] += class_mask\n",
    "\n",
    "            # Store results for current crop size\n",
    "            big_mask_logits_list.append(big_mask_logits)\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if os.path.exists(temp_dir):\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning temp directory: {e}\")\n",
    "\n",
    "    # Combine results from different crop sizes\n",
    "    if big_mask_logits_list:\n",
    "        # Take maximum values across all crop sizes\n",
    "        final_big_mask_logits = torch.stack(big_mask_logits_list).max(dim=0).values\n",
    "    else:\n",
    "        final_big_mask_logits = torch.zeros(\n",
    "            (num_classes, image_large.shape[0], image_large.shape[1]), \n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    m = final_big_mask_logits  # Store in global variable\n",
    "    \n",
    "    # Generate visualization results\n",
    "    results = visualize_results(final_big_mask_logits, image_large, class_names, risk_thresholds)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5919816d-41aa-45af-82b5-bc7dda6cad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the legend HTML with CSS (matching Gradio's style)\n",
    "legend_html = \"\"\"\n",
    "<div style=\"border: 1px solid #e0e0e0; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "    <h3 style=\"margin-top: 0;\">Legend for Boundary</h3>\n",
    "    <div style=\"display: flex; gap: 30px;\">\n",
    "        <!-- Left: Class -->\n",
    "        <div style=\"display: flex; flex-direction: column; gap: 10px;\">\n",
    "            <h4 style=\"margin: 0;\">Class</h4>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 4px; background-color: rgb(7, 222, 255); margin-right: 10px;\"></div>\n",
    "                <span>Seepage</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 4px; background-color: rgb(255, 207, 61); margin-right: 10px;\"></div>\n",
    "                <span>Corrosion</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 4px; background-color: rgb(75, 74, 255); margin-right: 10px;\"></div>\n",
    "                <span>Damaged Joint</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 4px; background-color: rgb(0, 85, 127); margin-right: 10px;\"></div>\n",
    "                <span>Spalling</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 4px; background-color: rgb(255, 15, 87); margin-right: 10px;\"></div>\n",
    "                <span>Crack</span>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <!-- Right: Risk Levels -->\n",
    "        <div style=\"display: flex; flex-direction: column; gap: 10px;\">\n",
    "            <h4 style=\"margin: 0;\">Risk Levels</h4>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 20px; border: 4px solid green; background-color: transparent; margin-right: 10px; border-radius: 4px;\"></div>\n",
    "                <span>Low Risk</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 20px; border: 4px solid orange; background-color: transparent; margin-right: 10px; border-radius: 4px;\"></div>\n",
    "                <span>Medium Risk</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 20px; border: 4px solid red; background-color: transparent; margin-right: 10px; border-radius: 4px;\"></div>\n",
    "                <span>High Risk</span>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138487f-b7c1-43b6-ab28-9cde710daff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from fpdf import FPDF\n",
    "from PyPDF2 import PdfWriter, PdfReader, PageObject\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def export_to_pdf():\n",
    "    if not results_cache:\n",
    "        return \"No images processed to export.\"\n",
    "\n",
    "    # Save boundary images to temporary files (with original filenames)\n",
    "    temp_files = []\n",
    "    try:\n",
    "        for file_name, result in results_cache.items():\n",
    "            boundary_image = result.get(\"boundary\")\n",
    "            if boundary_image:\n",
    "                # Convert image to RGB\n",
    "                boundary_image = boundary_image.convert(\"RGB\")\n",
    "                \n",
    "                # Get image dimensions\n",
    "                width, height = boundary_image.size\n",
    "\n",
    "                # Define the size of the extended area\n",
    "                margin_left = 100  # Left margin for vertical axis\n",
    "                margin_bottom = 50  # Bottom margin for horizontal axis\n",
    "\n",
    "                # Create a new image with extended area\n",
    "                new_width = width + margin_left\n",
    "                new_height = height + margin_bottom\n",
    "                new_image = Image.new(\"RGB\", (new_width, new_height), color=\"white\")\n",
    "\n",
    "                # Paste the original image into the new image\n",
    "                new_image.paste(boundary_image, (margin_left, 0))\n",
    "\n",
    "                # Draw coordinates on the new image\n",
    "                draw = ImageDraw.Draw(new_image)\n",
    "\n",
    "                # Load a larger font (adjust path if necessary)\n",
    "                try:\n",
    "                    font = ImageFont.truetype(\"arial.ttf\", size=50)  # Larger font size\n",
    "                except:\n",
    "                    font = ImageFont.load_default()  # Fallback to default font\n",
    "                    font.size = 50  # Adjust default font size\n",
    "\n",
    "                # Calculate the scale factor for vertical axis\n",
    "                # Horizontal axis: 0m at bottom, 20m at top\n",
    "                scale_factor = height / 20  # 1m = scale_factor pixels\n",
    "\n",
    "                # Draw horizontal axis (bottom to top: 0m to 20m)\n",
    "                for i in range(0, 21):  # From 0m to 20m\n",
    "                    y = height - int(i * scale_factor)  # Calculate y position (bottom to top)\n",
    "                    \n",
    "                    # Draw thicker dashed line\n",
    "                    dash_length = 5\n",
    "                    for x in range(margin_left, new_width, dash_length * 2):\n",
    "                        draw.line((x, y, x + dash_length, y), fill=\"gray\", width=2)  # Thicker line\n",
    "                    \n",
    "                    # Label the axis with larger text\n",
    "                    label = f\"{i}m\"\n",
    "                    draw.text((0, y), label, fill=\"black\", font=font)  # Adjust text position\n",
    "\n",
    "                # Draw vertical axis (center is 0, left to right)\n",
    "                center_x = width // 2 + margin_left  # Center of the original image\n",
    "                vertical_scale = scale_factor  # Use the same scale as horizontal axis\n",
    "\n",
    "                # 1. Draw center line and label \"0m\"\n",
    "                # Draw red center line\n",
    "                dash_length = 5\n",
    "                for y in range(0, new_height, dash_length * 2):\n",
    "                    draw.line((center_x, y, center_x, y + dash_length), fill=\"red\", width=2)\n",
    "                # Label \"0m\"\n",
    "                draw.text((center_x + 5, new_height - 50), \"0\", fill=\"black\", font=font)\n",
    "\n",
    "                # 2. Generate right-side ticks (+1m, +2m)\n",
    "                x = center_x + int(vertical_scale)  # Start from the first tick to the right of the center\n",
    "                while x < new_width:\n",
    "                    # Draw tick line\n",
    "                    for y in range(0, new_height, dash_length * 2):\n",
    "                        draw.line((x, y, x, y + dash_length), fill=\"gray\", width=2)\n",
    "                    # Calculate offset and label\n",
    "                    offset = int((x - center_x) / vertical_scale)\n",
    "                    draw.text((x + 5, new_height - 50), f\"{offset+1}\", fill=\"black\", font=font)\n",
    "                    x += int(vertical_scale)\n",
    "\n",
    "                # 3. Generate left-side ticks (-1m, -2m)\n",
    "                x = center_x - int(vertical_scale)  # Start from the first tick to the left of the center\n",
    "                while x > margin_left:\n",
    "                    # Draw tick line\n",
    "                    for y in range(0, new_height, dash_length * 2):\n",
    "                        draw.line((x, y, x, y + dash_length), fill=\"gray\", width=2)\n",
    "                    # Calculate offset and label\n",
    "                    offset = int((x - center_x) / vertical_scale)\n",
    "                    draw.text((x + 5, new_height - 50), f\"{offset-1}\", fill=\"black\", font=font)\n",
    "                    x -= int(vertical_scale)\n",
    "\n",
    "                # Save the new image to a temporary file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\") as temp_file:\n",
    "                    temp_path = temp_file.name\n",
    "                    new_image.save(temp_path)\n",
    "                    temp_files.append((temp_path, file_name))  # Store both temp path and file name\n",
    "\n",
    "        # Load the PDF template\n",
    "        template_path = \"template.pdf\"  # Path to your PDF template\n",
    "        if not os.path.exists(template_path):\n",
    "            return \"Template PDF not found.\"\n",
    "\n",
    "        # Create a PDF writer\n",
    "        writer = PdfWriter()\n",
    "\n",
    "        # Read the template PDF\n",
    "        template_pdf = PdfReader(template_path)\n",
    "        template_page = template_pdf.pages[0]  # Original template page\n",
    "\n",
    "        # Define the position for the boundary image\n",
    "        boundary_x = 10  # X position in mm\n",
    "        boundary_y = 48  # Y position in mm (from top)\n",
    "        target_width = 170  # Target width in mm\n",
    "        target_height = 220  # Target height in mm\n",
    "\n",
    "        # Convert mm to points (1 mm = 2.83465 points)\n",
    "        boundary_x_pt = boundary_x * 2.83465\n",
    "        boundary_y_pt = (297 - boundary_y - target_height) * 2.83465  # Convert Y coordinate from top to bottom\n",
    "        target_width_pt = target_width * 2.83465\n",
    "        target_height_pt = target_height * 2.83465\n",
    "\n",
    "        # Add each boundary image to a new page\n",
    "        for temp_path, file_name in temp_files:\n",
    "            # Create a new blank page and merge the template content\n",
    "            new_page = PageObject.create_blank_page(width=template_page.mediabox.width, height=template_page.mediabox.height)\n",
    "            new_page.merge_page(template_page)  # Copy template content to the new page\n",
    "\n",
    "            # Read the boundary image to get its dimensions\n",
    "            with Image.open(temp_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "\n",
    "            # Calculate the aspect ratio of the boundary image\n",
    "            aspect_ratio = img_width / img_height\n",
    "\n",
    "            # Calculate the scaled dimensions to fit within the target area\n",
    "            if aspect_ratio > (target_width / target_height):\n",
    "                # Fit to width\n",
    "                scaled_width = target_width_pt\n",
    "                scaled_height = scaled_width / aspect_ratio\n",
    "            else:\n",
    "                # Fit to height\n",
    "                scaled_height = target_height_pt\n",
    "                scaled_width = scaled_height * aspect_ratio\n",
    "\n",
    "            # Create a new FPDF object for the boundary image\n",
    "            boundary_pdf = FPDF()\n",
    "            boundary_pdf.add_page()\n",
    "\n",
    "            # Add the image to the boundary_pdf\n",
    "            boundary_pdf.image(\n",
    "                temp_path,\n",
    "                x=boundary_x,\n",
    "                y=boundary_y,\n",
    "                w=scaled_width / 2.83465,\n",
    "                h=scaled_height / 2.83465\n",
    "            )\n",
    "\n",
    "            # Add the corresponding file name to the bottom-left corner\n",
    "            filename_without_ext = os.path.splitext(file_name)[0]  # Remove file extension\n",
    "            \n",
    "            # Check if the filename is too long and truncate if necessary\n",
    "            max_filename_length = 30  # Maximum characters to display\n",
    "            if len(filename_without_ext) > max_filename_length:\n",
    "                filename_without_ext = filename_without_ext[:max_filename_length - 3] + \"...\"\n",
    "            \n",
    "            # Set font and size for the filename\n",
    "            boundary_pdf.set_font(\"helvetica\", size=8)  # Set font and size\n",
    "            \n",
    "            # Calculate the position for the filename (left-bottom corner)\n",
    "            filename_x = 8  # 8 mm from the left edge\n",
    "            filename_y = 290  # 290 mm from the top edge (near the bottom)\n",
    "            \n",
    "            # Draw the filename on the PDF\n",
    "            boundary_pdf.text(filename_x, filename_y, filename_without_ext)\n",
    "\n",
    "            # Save the temporary PDF for the current page\n",
    "            boundary_pdf_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\").name\n",
    "            boundary_pdf.output(boundary_pdf_path)\n",
    "\n",
    "            # Merge the boundary image PDF with the new page\n",
    "            boundary_pdf_page = PdfReader(boundary_pdf_path).pages[0]\n",
    "            boundary_pdf_page.add_transformation([1, 0, 0, 1, boundary_x_pt, boundary_y_pt])\n",
    "            new_page.merge_page(boundary_pdf_page)\n",
    "\n",
    "            # Add the merged page to the writer\n",
    "            writer.add_page(new_page)\n",
    "\n",
    "        # Save the final PDF\n",
    "        output_path = \"tunnel_damage_report.pdf\"\n",
    "        with open(output_path, \"wb\") as output_file:\n",
    "            writer.write(output_file)\n",
    "\n",
    "        return output_path\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for temp_path, _ in temp_files:\n",
    "            if os.path.exists(temp_path):\n",
    "                os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95f971-b3ed-435b-90b3-860d63c7bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def export_annotations():\n",
    "    \"\"\"\n",
    "    Generate COCO-format annotation.json file using saved \"json_coords\" data from results_cache.\n",
    "    The file is saved in ./annotations/annotation.json and its path is returned.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the generated annotation file or error message if no data available.\n",
    "    \"\"\"\n",
    "    global results_cache, class_names\n",
    "\n",
    "    # Check if there's any processed data\n",
    "    if not results_cache:\n",
    "        return \"No images processed to export annotations.\"\n",
    "\n",
    "    # Initialize COCO format data structure\n",
    "    coco_format = {\n",
    "        \"info\": {},            # Metadata about the dataset\n",
    "        \"licenses\": [],        # License information\n",
    "        \"images\": [],          # List of image information\n",
    "        \"annotations\": [],     # List of all annotations\n",
    "        \"categories\": []       # List of object categories\n",
    "    }\n",
    "\n",
    "    # Add category information (COCO uses 1-based indexing)\n",
    "    for i, name in enumerate(class_names):\n",
    "        coco_format[\"categories\"].append({\n",
    "            \"id\": i + 1,           # Category ID (starting from 1)\n",
    "            \"name\": name,          # Category name\n",
    "            \"supercategory\": \"none\" # No supercategory\n",
    "        })\n",
    "\n",
    "    # Initialize counters for unique IDs\n",
    "    annotation_id = 1  # Global annotation counter\n",
    "    image_id = 1       # Image counter (starting from 1)\n",
    "\n",
    "    # Process each image in the results cache\n",
    "    for file_name, result in results_cache.items():\n",
    "        original = result.get(\"original\")\n",
    "        if original is None:\n",
    "            continue\n",
    "            \n",
    "        # Get image dimensions\n",
    "        h, w = original.shape[:2]\n",
    "        \n",
    "        # Add image information\n",
    "        coco_format[\"images\"].append({\n",
    "            \"id\": image_id,        # Unique image ID\n",
    "            \"file_name\": file_name, # Original filename\n",
    "            \"width\": w,            # Image width\n",
    "            \"height\": h            # Image height\n",
    "        })\n",
    "        \n",
    "        # Add all annotations for this image\n",
    "        for ann in result.get(\"json_coords\", []):\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,          # Unique annotation ID\n",
    "                \"image_id\": image_id,          # Reference to image\n",
    "                \"category_id\": ann[\"class_id\"], # Object category\n",
    "                \"segmentation\": [ann[\"segmentation\"]], # Polygon coordinates\n",
    "                \"bbox\": ann[\"bbox\"],           # [x,y,width,height] format\n",
    "                \"area\": ann[\"area\"],            # Pixel area of the object\n",
    "                \"iscrowd\": 0                    # 0 for individual instances\n",
    "            })\n",
    "            annotation_id += 1  # Increment annotation counter\n",
    "\n",
    "        image_id += 1  # Increment image counter\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = \"./annotations\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save JSON file with pretty formatting\n",
    "    output_path = os.path.join(output_dir, \"annotation.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e9294-a109-443b-a34c-8f1d12f8ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"\")\n",
    "\n",
    "# Define paths for original and processed annotation files\n",
    "original_json_path = \"annotations/annotation.json\"\n",
    "processed_json_path = \"annotations/processed_annotation.json\"\n",
    "\n",
    "# Cache for processed COCO data to avoid redundant file reads\n",
    "cached_coco_json = None\n",
    "\n",
    "def preprocess_coco_json():\n",
    "    \"\"\" \n",
    "    Load COCO JSON, remove segmentation field, and save as a new file if not already processed. \n",
    "    \"\"\"\n",
    "    global cached_coco_json\n",
    "\n",
    "    if cached_coco_json is not None:\n",
    "        return cached_coco_json\n",
    "\n",
    "    if os.path.exists(processed_json_path):\n",
    "        with open(processed_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_coco_json = json.load(f)\n",
    "        return cached_coco_json\n",
    "\n",
    "    if os.path.exists(original_json_path):\n",
    "        try:\n",
    "            with open(original_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                coco_json = json.load(f)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(original_json_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "                coco_json = json.load(f)\n",
    "\n",
    "        # remove segmentation\n",
    "        for annotation in coco_json.get(\"annotations\", []):\n",
    "            annotation.pop(\"segmentation\", None)\n",
    "\n",
    "        # save processed JSON\n",
    "        with open(processed_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(coco_json, f, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "\n",
    "        cached_coco_json = coco_json\n",
    "        return cached_coco_json\n",
    "\n",
    "    return None\n",
    "\n",
    "def clean_generated_code(code):\n",
    "    \"\"\"\n",
    "    Clean GPT-generated Python code.\n",
    "    \"\"\"\n",
    "    code = code.strip()\n",
    "    if code.startswith(\"```\"):\n",
    "        code = \"\\n\".join(code.splitlines()[1:])\n",
    "    if code.endswith(\"```\"):\n",
    "        code = \"\\n\".join(code.splitlines()[:-1])\n",
    "    return code.strip()\n",
    "\n",
    "def analyze_coco_with_chatgpt(user_input):\n",
    "    \"\"\"\n",
    "    Call OpenAI GPT to generate Python code for analyzing the COCO dataset and execute the code.\n",
    "    \"\"\"\n",
    "    coco_json = preprocess_coco_json()\n",
    "    if not coco_json:\n",
    "        return \"Error: annotations.json file not found\", None\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a data analysis assistant specialized in analyzing COCO-format JSON annotation files. The files contain three main sections: images, annotations, and categories.\n",
    "    Your task is to generate Python code to perform statistical analysis based on user queries.\n",
    "\n",
    "    Rules:\n",
    "    1. Only generate statistics based on the COCO JSON file. Do not answer questions beyond this scope.\n",
    "    2. The generated code must be written in Python using `json`, `collections`, and `matplotlib` for statistics and visualization.\n",
    "    3. The code must be self-contained and dynamically compute all necessary variables from `\"annotations/processed_annotation.json\"`.\n",
    "    4. Store the statistical results in the `result` variable and any Matplotlib figure in the `fig` variable.\n",
    "    5. Operate `result` and `fig` at the global scope. Do not define them inside functions or classes.\n",
    "    6. Ensure all required libraries (e.g., `json`, `matplotlib.pyplot`) are explicitly imported at the beginning of the code.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"User query: {user_input}. Please generate Python code.\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI API call failed: {str(e)}\", None\n",
    "\n",
    "    generated_code = response.choices[0].message.content.strip()\n",
    "    generated_code = clean_generated_code(generated_code)\n",
    "    print(\"Generated Code:\\n\", generated_code)\n",
    "\n",
    "    local_vars = {\n",
    "        \"json_data\": coco_json,\n",
    "        \"result\": None,\n",
    "        \"fig\": None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        exec(generated_code, local_vars, local_vars)\n",
    "    except Exception as e:\n",
    "        return f\"Code execution error: {str(e)}\", None\n",
    "\n",
    "    result = local_vars.get(\"result\", \"Unable to retrieve results\")\n",
    "    fig = local_vars.get(\"fig\", None)\n",
    "\n",
    "    img_path = None\n",
    "    if fig:\n",
    "        img_path = os.path.join(tempfile.gettempdir(), \"chart.png\")\n",
    "        try:\n",
    "            fig.savefig(img_path)\n",
    "        except Exception as e:\n",
    "            return f\"Failed to save chart: {str(e)}\", None\n",
    "\n",
    "    return result, img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40f8d0d3-e173-426e-9cd9-2b4d8a96fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:13<00:00,  1.73it/s]\n",
      "100%|██████████| 12/12 [00:10<00:00,  1.19it/s]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tempfile\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Tunnel panoramic image damage detection\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left side: controls and download (0.2 width)\n",
    "        with gr.Column(scale=1):  \n",
    "            image_input = gr.File(\n",
    "                label=\"Upload Images\",\n",
    "                file_count=\"multiple\",\n",
    "                file_types=[\"image\"]\n",
    "            )\n",
    "            confidence_slider = gr.Slider(\n",
    "                0, 1, 0.2, \n",
    "                step=0.01, \n",
    "                label=\"Confidence Threshold\"\n",
    "            )\n",
    "            confidence_slider_crack = gr.Slider(\n",
    "                0, 1, 0.05, \n",
    "                step=0.01, \n",
    "                label=\"Confidence Threshold (Crack)\"\n",
    "            )\n",
    "            submit_button = gr.Button(\"Process Images\")\n",
    "        \n",
    "            image_selector = gr.Dropdown(\n",
    "                choices=[],\n",
    "                label=\"Select Image\",\n",
    "                interactive=True\n",
    "            )\n",
    "            \n",
    "            # Add the legend section above the export button\n",
    "            gr.HTML(legend_html)\n",
    "\n",
    "            # Export annotation button and download link\n",
    "            export_annotations_button = gr.Button(\"Export Annotations\")\n",
    "            download_annotations_link = gr.File(label=\"Download Annotations\", file_types=[\".json\"])\n",
    "            \n",
    "            # Export pdf button and download link\n",
    "            export_pdf_button = gr.Button(\"Export Boundaries to PDF\")\n",
    "            download_pdf_link = gr.File(label=\"Download PDF\")\n",
    "\n",
    "        # Middle and right side: Visualization Type, Original Image, and Visualization Result\n",
    "        with gr.Column(scale=4):\n",
    "            # Visualization Type moved to a new row above the images\n",
    "            with gr.Row():\n",
    "                output_choice = gr.Radio(\n",
    "                    choices=[\"Segmentation\", \"Probability\", \"Binary\", \"Boundary\"],\n",
    "                    label=\"Visualization Type\",\n",
    "                    value=\"Segmentation\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Middle side: original image (0.4 width)\n",
    "                with gr.Column(scale=1):  \n",
    "                    original_display = gr.Image(label=\"Original Image\")\n",
    "                \n",
    "                # Right side: output display (0.4 width)\n",
    "                with gr.Column(scale=1):  \n",
    "                    output_display = gr.Image(label=\"Visualization Result\")\n",
    "\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Statistical information query\")\n",
    "                    user_chat_input = gr.Textbox(label=\"Enter your question\", placeholder=\"How many types of damage did we predict?\")\n",
    "                    chat_submit_button = gr.Button(\"Query Statistics\")\n",
    "                    chat_output = gr.Textbox(label=\"Statistical results\", interactive=False)\n",
    "                with gr.Column(scale=1):\n",
    "                    chart_display = gr.Image(label=\"Statistical Charts\", interactive=False)\n",
    "        \n",
    "    results_cache = {}  # Store processed results\n",
    "\n",
    "    def process_batch(files, confidence_threshold, crack_threshold):\n",
    "        global results_cache\n",
    "        results_cache.clear()\n",
    "        file_names = []\n",
    "        \n",
    "        # Process each uploaded image\n",
    "        for file in files:\n",
    "            file_name = os.path.basename(file.name)\n",
    "            file_names.append(file_name)\n",
    "            \n",
    "            image = Image.open(file.name)\n",
    "            img_array = np.array(image)\n",
    "            result = process_image(img_array, confidence_threshold, crack_threshold)\n",
    "            results_cache[file_name] = result\n",
    "        \n",
    "        # Update the image_selector choices dynamically\n",
    "        return gr.update(choices=file_names), results_cache.get(file_names[0], {}).get(\"original\", None), results_cache.get(file_names[0], {}).get(\"segmentation\", None)\n",
    "    \n",
    "    def update_display(file_name, viz_type):\n",
    "        if not file_name or file_name not in results_cache:\n",
    "            return None, None\n",
    "        result = results_cache[file_name]\n",
    "        \n",
    "        # Return the correct visualization based on the selected type\n",
    "        original_image = result.get(\"original\", None)\n",
    "        if viz_type == \"Segmentation\":\n",
    "            return original_image, result.get(\"segmentation\", None)\n",
    "        elif viz_type == \"Probability\":\n",
    "            return original_image, result.get(\"probability\", None)\n",
    "        elif viz_type == \"Binary\":\n",
    "            return original_image, result.get(\"binary\", None)\n",
    "        elif viz_type == \"Boundary\":\n",
    "            return original_image, result.get(\"boundary\", None)\n",
    "        return None, None\n",
    "    \n",
    "    # Connect components\n",
    "    submit_button.click(\n",
    "        fn=process_batch,\n",
    "        inputs=[image_input, confidence_slider, confidence_slider_crack],\n",
    "        outputs=[image_selector, original_display, output_display]\n",
    "    )\n",
    "    \n",
    "    image_selector.change(\n",
    "        fn=update_display,\n",
    "        inputs=[image_selector, output_choice],\n",
    "        outputs=[original_display, output_display]\n",
    "    )\n",
    "    \n",
    "    output_choice.change(\n",
    "        fn=update_display,\n",
    "        inputs=[image_selector, output_choice],\n",
    "        outputs=[original_display, output_display]\n",
    "    )\n",
    "\n",
    "    # Connect export button\n",
    "    export_annotations_button.click(\n",
    "        fn=export_annotations,\n",
    "        inputs=[],\n",
    "        outputs=download_annotations_link\n",
    "    )\n",
    "    export_pdf_button.click(\n",
    "        fn=export_to_pdf,\n",
    "        inputs=[],\n",
    "        outputs=download_pdf_link\n",
    "    )\n",
    "\n",
    "    # llm interface\n",
    "    chat_submit_button.click(\n",
    "        fn=analyze_coco_with_chatgpt,\n",
    "        inputs=[user_chat_input],\n",
    "        outputs=[chat_output, chart_display]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62f0b6-67af-4fac-bdd6-f979b017ecfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
